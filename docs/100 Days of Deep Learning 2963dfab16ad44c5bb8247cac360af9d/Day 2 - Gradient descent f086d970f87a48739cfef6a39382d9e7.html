<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Day 2 - Gradient descent</title>
    <link rel="stylesheet" href="../styles.css" />
  </head>
  <body>
    <article id="f086d970-f87a-4873-9cfe-f6a39382d9e7" class="page sans">
      <header><h1 class="page-title">Day 2 - Gradient descent</h1></header>
      <div class="page-body">
        <h1 id="28d5ded7-f617-4633-b79c-b2f70e155abf" class="">
          Gradient Descent
        </h1>
        <ul id="d8d44b32-ea24-41ac-a6fc-acfe042c2aee" class="bulleted-list">
          <li>
            Gradient descent is an <strong>optimization algorithm</strong> that
            <strong>iteratively</strong> tries to find the
            <strong>best parameters</strong> for a model.
          </li>
        </ul>
        <ul id="e3ff5fd0-d848-4785-9c2d-47699fa3eb6c" class="bulleted-list">
          <li>
            We know how good (or bad) the parameters are by looking at the value
            of the <strong>cost function</strong>.
          </li>
        </ul>
        <h2 id="c0104704-c713-4ca1-8a0f-a4defdcc7adc" class="">
          The Algorithm
        </h2>
        <ol
          id="138c0951-54a3-456f-859a-bad3a5c98fb5"
          class="numbered-list"
          start="1"
        >
          <li>Start with random values for the parameters.</li>
        </ol>
        <ol
          id="34aea418-bf7d-4f05-bb9c-017488ae8a34"
          class="numbered-list"
          start="2"
        >
          <li>
            Measure the gradient of the cost function with respect to the
            parameters.
          </li>
        </ol>
        <ol
          id="4054808c-6ac0-4311-9b71-a7183feddba4"
          class="numbered-list"
          start="3"
        >
          <li>
            Update the parameters in the direction of descending gradient to
            decrease the cost function.
          </li>
        </ol>
        <p id="cf39619d-b5be-4f22-81a1-3eceb25fa1c4" class="">
          Steps 2 &amp; 3 are repeated until the best parameters are found (the
          algorithm converges/the gloabl minimum of the cost function is
          reached).
        </p>
        <h2 id="05510b78-d6ed-480f-9a23-dcf8b29cb6fb" class="">
          Learning Rate
        </h2>
        <ul id="166428c4-2ac2-4d1c-871c-986482e85630" class="bulleted-list">
          <li>
            How large a change you make to the parameters in step 3 depends on
            the <strong>learning rate</strong>.
          </li>
        </ul>
        <ul id="ece0f2fe-88c5-4a6a-8614-aa1b558be9a5" class="bulleted-list">
          <li>
            If the learning rate is too small, the algorithm will take too long
            to converge.
          </li>
        </ul>
        <ul id="da2b70d7-f25a-416e-95c7-9da573fbd272" class="bulleted-list">
          <li>
            If the learning rate is too big, the algorithm will overshoot the
            minima and might even end up diverging.
          </li>
        </ul>
        <h2 id="14f8b7a7-6ee2-4341-9c6e-180a4deed6e9" class="">
          Types of Gradient Descent
        </h2>
        <ol
          id="8896de9e-a520-4b18-8d27-4415cf8546b9"
          class="numbered-list"
          start="1"
        >
          <li>Batch gradient descent</li>
        </ol>
        <ol
          id="6156973e-4eaf-4f63-a4e3-d901f28485b7"
          class="numbered-list"
          start="2"
        >
          <li>Stochastic gradient descent</li>
        </ol>
        <ol
          id="313f63fe-64f0-4302-b863-d94fa949ab60"
          class="numbered-list"
          start="3"
        >
          <li>Mini-batch gradient descent</li>
        </ol>
        <div
          id="1f11ae97-6b65-4801-9a7e-b49da475d909"
          class="collection-content"
        >
          <h4 class="collection-title">Comparison of gradient descent types</h4>
          <table class="collection-content">
            <thead>
              <tr>
                <th style="width: 20%">Property</th>
                <th style="width: 20%">Batch</th>
                <th>Stochastic</th>
                <th>Mini-batch</th>
              </tr>
            </thead>
            <tbody>
              <tr id="2b782137-f9c1-4a47-9ef6-99df34f7e071">
                <td class="cell-title">
                  Amount of data used in each gradient descent step
                </td>
                <td class="cell-`@Ai">Entire dataset</td>
                <td class="cell-b`WM">One sample</td>
                <td class="cell-t_Jt">Fixed random number of samples</td>
              </tr>
              <tr id="a694348e-3061-40da-b873-e993cbc286e4">
                <td class="cell-title">Speed</td>
                <td class="cell-`@Ai">Slow</td>
                <td class="cell-b`WM">Fast</td>
                <td class="cell-t_Jt">Fast on GPU</td>
              </tr>
              <tr id="ce4b47ba-b631-4bcf-8f7b-d7cc7ef77702">
                <td class="cell-title">Memory requirement</td>
                <td class="cell-`@Ai">High</td>
                <td class="cell-b`WM">Low</td>
                <td class="cell-t_Jt">Depends on batch size</td>
              </tr>
              <tr id="c9f73169-defc-450c-b8f3-055ad34083e3">
                <td class="cell-title">Shuffling of data required</td>
                <td class="cell-`@Ai">No</td>
                <td class="cell-b`WM">
                  Yes, so that data is Independent and Identically Distributed
                  (IID) and the algorithm goes towards the global minimum on
                  average instead of trying to optimize for one label and then
                  the next in the case of data being sorted by label
                </td>
                <td class="cell-t_Jt">Yes</td>
              </tr>
              <tr id="a797975b-1663-49da-af0c-5141371ef3b5">
                <td class="cell-title">Disadvantages</td>
                <td class="cell-`@Ai"></td>
                <td class="cell-b`WM">
                  Since entire dataset is not used, gradient updates are not
                  completely accurate (in direction of steepest descent of cost
                  function) Shuffling data and going through each instance slows
                  down convergence
                </td>
                <td class="cell-t_Jt">
                  More prone to getting stuck in local minima or plateau
                  compared to stochastic gradient descent
                </td>
              </tr>
              <tr id="d20b2bf4-3cb3-499b-a0e3-4c8bd4741306">
                <td class="cell-title">Advantages</td>
                <td class="cell-`@Ai">Scales well with number of features</td>
                <td class="cell-b`WM">
                  Works well for really large datasets (since only one instance
                  has to be kept in memory at a time) Since gradient descent
                  updates are not in direction of steepest descent, it has the
                  ability to jump out of a local minima
                </td>
                <td class="cell-t_Jt">
                  Less erratic than stochastic gradient descent
                </td>
              </tr>
              <tr id="31cb567f-1949-4cc5-925c-503ec3c37871">
                <td class="cell-title">Learning Schedule</td>
                <td class="cell-`@Ai"></td>
                <td class="cell-b`WM"></td>
                <td class="cell-t_Jt">
                  Start with a large learning rate to enable algorithm to escape
                  local minima, gradually reduce it to allow algorithm to settle
                  at global minimum
                </td>
              </tr>
            </tbody>
          </table>
        </div>
        <p id="dc70382b-4536-4617-bb96-27d51ebf2e04" class=""></p>
        <ul id="25024943-7ab4-4141-aa00-84b3a00e6342" class="bulleted-list">
          <li>
            Gradient descent converges faster if all features have a similar
            scale.
          </li>
        </ul>
        <ul id="430c008b-9c90-4a03-9300-00e946dfd3aa" class="bulleted-list">
          <li>
            A common practice while using gradient descent is early stopping -
            start training for a large number of iterations and stop when the
            gradient vector becomes very small. This happens when gradient
            descent has almost reached the minimum.
          </li>
        </ul>
        <p id="a32370a8-d4d4-46d7-98dd-2e516d348b1f" class=""></p>
      </div>
    </article>
  </body>
</html>
