{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom Training Loop in TensorFlow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1TZYeCZ2+5LtQobRfZLDd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravimashru/100-days-of-deep-learning/blob/master/docs/days/029_Custom_Training_Loop_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrPZWUnUaX3M"
      },
      "source": [
        "Reference: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ0iERJOaUvv",
        "outputId": "10269330-7956-4cc3-b8ba-69ffa4b8fca2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print('TensorFlow: ', tf.__version__)\n",
        "print('NumPy: ', np.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow:  2.3.0\n",
            "NumPy:  1.18.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewI6VZkIbcNa",
        "outputId": "6a7fab3b-3689-45bf-fdbb-62505b5fa213"
      },
      "source": [
        "handwritten_digits_mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = handwritten_digits_mnist.load_data()\n",
        "\n",
        "print('Training data: ', train_images.shape, train_labels.shape)\n",
        "print('Test data: ', test_images.shape, test_labels.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Training data:  (60000, 28, 28) (60000,)\n",
            "Test data:  (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcrawejgbePS",
        "outputId": "811e3810-8d78-49b1-c7bf-0b87ec321a43"
      },
      "source": [
        "discriminator = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(28, 28, 1)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.GlobalMaxPooling2D(),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "discriminator.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 14, 14, 64)        640       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 74,625\n",
            "Trainable params: 74,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4XiybWrbhvE",
        "outputId": "91aa6b29-9b7e-4426-9f84-d166054388b2"
      },
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(latent_dim,)),\n",
        "        tf.keras.layers.Dense(7 * 7 * 128),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Reshape((7, 7, 128)),\n",
        "        tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
        "        tf.keras.layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")\n",
        "generator.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 6272)              809088    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 1)         6273      \n",
            "=================================================================\n",
            "Total params: 1,339,905\n",
            "Trainable params: 1,339,905\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVF3oXFpb2kd"
      },
      "source": [
        "# Instantiate one optimizer for the discriminator and another for the generator.\n",
        "d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
        "g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004)\n",
        "\n",
        "# Instantiate a loss function.\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_images):\n",
        "    # Sample random points in the latent space\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "    # Decode them to fake images\n",
        "    generated_images = generator(random_latent_vectors)\n",
        "    # Combine them with real images\n",
        "    combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "\n",
        "    # Assemble labels discriminating real from fake images\n",
        "    labels = tf.concat(\n",
        "        [tf.ones((batch_size, 1)), tf.zeros((real_images.shape[0], 1))], axis=0\n",
        "    )\n",
        "    # Add random noise to the labels - important trick!\n",
        "    labels += 0.05 * tf.random.uniform(labels.shape)\n",
        "\n",
        "    # Train the discriminator\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(combined_images)\n",
        "        d_loss = loss_fn(labels, predictions)\n",
        "    grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
        "    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "\n",
        "    # Sample random points in the latent space\n",
        "    random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
        "    # Assemble labels that say \"all real images\"\n",
        "    misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "    # Train the generator (note that we should *not* update the weights\n",
        "    # of the discriminator)!\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = discriminator(generator(random_latent_vectors))\n",
        "        g_loss = loss_fn(misleading_labels, predictions)\n",
        "    grads = tape.gradient(g_loss, generator.trainable_weights)\n",
        "    g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "    return d_loss, g_loss, generated_images\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6INlwZ--cE9c",
        "outputId": "8cab06fc-ac27-4358-ed68-10a36163347b"
      },
      "source": [
        "import os\n",
        "\n",
        "# Prepare the dataset. We use both the training & test MNIST digits.\n",
        "batch_size = 64\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
        "all_digits = np.concatenate([x_train, x_test])\n",
        "all_digits = all_digits.astype(\"float32\") / 255.0\n",
        "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "epochs = 20  # In practice you need at least 20 epochs to generate nice digits.\n",
        "save_dir = \"./\"\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart epoch\", epoch)\n",
        "\n",
        "    for step, real_images in enumerate(dataset):\n",
        "        # Train the discriminator & generator on one batch of real images.\n",
        "        d_loss, g_loss, generated_images = train_step(real_images)\n",
        "\n",
        "        # Logging.\n",
        "        if step % 200 == 0:\n",
        "            # Print metrics\n",
        "            print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))\n",
        "            print(\"adversarial loss at step %d: %.2f\" % (step, g_loss))\n",
        "\n",
        "            # Save one generated image\n",
        "            img = tf.keras.preprocessing.image.array_to_img(\n",
        "                generated_images[0] * 255.0, scale=False\n",
        "            )\n",
        "            img.save(os.path.join(save_dir, \"generated_img\" + str(step) + \".png\"))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start epoch 0\n",
            "discriminator loss at step 0: 0.71\n",
            "adversarial loss at step 0: 0.70\n",
            "discriminator loss at step 200: 0.20\n",
            "adversarial loss at step 200: 2.77\n",
            "discriminator loss at step 400: 0.51\n",
            "adversarial loss at step 400: 1.27\n",
            "discriminator loss at step 600: 0.39\n",
            "adversarial loss at step 600: 1.58\n",
            "discriminator loss at step 800: 0.35\n",
            "adversarial loss at step 800: 1.84\n",
            "discriminator loss at step 1000: 0.37\n",
            "adversarial loss at step 1000: 1.41\n",
            "\n",
            "Start epoch 1\n",
            "discriminator loss at step 0: 0.21\n",
            "adversarial loss at step 0: 2.30\n",
            "discriminator loss at step 200: 0.20\n",
            "adversarial loss at step 200: 2.92\n",
            "discriminator loss at step 400: 0.19\n",
            "adversarial loss at step 400: 3.01\n",
            "discriminator loss at step 600: 0.15\n",
            "adversarial loss at step 600: 3.16\n",
            "discriminator loss at step 800: 0.06\n",
            "adversarial loss at step 800: 4.93\n",
            "discriminator loss at step 1000: 0.08\n",
            "adversarial loss at step 1000: 4.06\n",
            "\n",
            "Start epoch 2\n",
            "discriminator loss at step 0: 0.06\n",
            "adversarial loss at step 0: 4.10\n",
            "discriminator loss at step 200: 0.07\n",
            "adversarial loss at step 200: 3.75\n",
            "discriminator loss at step 400: 0.06\n",
            "adversarial loss at step 400: 4.89\n",
            "discriminator loss at step 600: 0.02\n",
            "adversarial loss at step 600: 5.92\n",
            "discriminator loss at step 800: 0.02\n",
            "adversarial loss at step 800: 5.31\n",
            "discriminator loss at step 1000: 0.04\n",
            "adversarial loss at step 1000: 6.00\n",
            "\n",
            "Start epoch 3\n",
            "discriminator loss at step 0: 0.11\n",
            "adversarial loss at step 0: 4.46\n",
            "discriminator loss at step 200: 0.60\n",
            "adversarial loss at step 200: 1.05\n",
            "discriminator loss at step 400: 0.49\n",
            "adversarial loss at step 400: 1.10\n",
            "discriminator loss at step 600: 0.54\n",
            "adversarial loss at step 600: 0.94\n",
            "discriminator loss at step 800: 0.57\n",
            "adversarial loss at step 800: 1.06\n",
            "discriminator loss at step 1000: 0.52\n",
            "adversarial loss at step 1000: 1.28\n",
            "\n",
            "Start epoch 4\n",
            "discriminator loss at step 0: 0.57\n",
            "adversarial loss at step 0: 1.10\n",
            "discriminator loss at step 200: 0.52\n",
            "adversarial loss at step 200: 1.20\n",
            "discriminator loss at step 400: 0.62\n",
            "adversarial loss at step 400: 0.76\n",
            "discriminator loss at step 600: 0.62\n",
            "adversarial loss at step 600: 0.94\n",
            "discriminator loss at step 800: 0.58\n",
            "adversarial loss at step 800: 1.02\n",
            "discriminator loss at step 1000: 0.49\n",
            "adversarial loss at step 1000: 1.21\n",
            "\n",
            "Start epoch 5\n",
            "discriminator loss at step 0: 0.58\n",
            "adversarial loss at step 0: 1.16\n",
            "discriminator loss at step 200: 0.57\n",
            "adversarial loss at step 200: 0.98\n",
            "discriminator loss at step 400: 0.48\n",
            "adversarial loss at step 400: 1.22\n",
            "discriminator loss at step 600: 0.57\n",
            "adversarial loss at step 600: 1.06\n",
            "discriminator loss at step 800: 0.73\n",
            "adversarial loss at step 800: 0.77\n",
            "discriminator loss at step 1000: 0.64\n",
            "adversarial loss at step 1000: 0.93\n",
            "\n",
            "Start epoch 6\n",
            "discriminator loss at step 0: 0.51\n",
            "adversarial loss at step 0: 1.24\n",
            "discriminator loss at step 200: 0.59\n",
            "adversarial loss at step 200: 1.04\n",
            "discriminator loss at step 400: 0.59\n",
            "adversarial loss at step 400: 1.05\n",
            "discriminator loss at step 600: 0.63\n",
            "adversarial loss at step 600: 1.13\n",
            "discriminator loss at step 800: 0.69\n",
            "adversarial loss at step 800: 0.83\n",
            "discriminator loss at step 1000: 0.52\n",
            "adversarial loss at step 1000: 1.20\n",
            "\n",
            "Start epoch 7\n",
            "discriminator loss at step 0: 0.60\n",
            "adversarial loss at step 0: 1.01\n",
            "discriminator loss at step 200: 0.46\n",
            "adversarial loss at step 200: 1.22\n",
            "discriminator loss at step 400: 0.66\n",
            "adversarial loss at step 400: 0.92\n",
            "discriminator loss at step 600: 0.75\n",
            "adversarial loss at step 600: 0.85\n",
            "discriminator loss at step 800: 0.43\n",
            "adversarial loss at step 800: 1.56\n",
            "discriminator loss at step 1000: 0.58\n",
            "adversarial loss at step 1000: 1.02\n",
            "\n",
            "Start epoch 8\n",
            "discriminator loss at step 0: 0.60\n",
            "adversarial loss at step 0: 1.01\n",
            "discriminator loss at step 200: 0.62\n",
            "adversarial loss at step 200: 1.05\n",
            "discriminator loss at step 400: 0.57\n",
            "adversarial loss at step 400: 1.19\n",
            "discriminator loss at step 600: 0.67\n",
            "adversarial loss at step 600: 0.80\n",
            "discriminator loss at step 800: 0.64\n",
            "adversarial loss at step 800: 0.98\n",
            "discriminator loss at step 1000: 0.56\n",
            "adversarial loss at step 1000: 0.90\n",
            "\n",
            "Start epoch 9\n",
            "discriminator loss at step 0: 0.55\n",
            "adversarial loss at step 0: 1.19\n",
            "discriminator loss at step 200: 0.63\n",
            "adversarial loss at step 200: 1.09\n",
            "discriminator loss at step 400: 0.66\n",
            "adversarial loss at step 400: 1.05\n",
            "discriminator loss at step 600: 0.64\n",
            "adversarial loss at step 600: 1.10\n",
            "discriminator loss at step 800: 0.60\n",
            "adversarial loss at step 800: 0.80\n",
            "discriminator loss at step 1000: 0.61\n",
            "adversarial loss at step 1000: 1.04\n",
            "\n",
            "Start epoch 10\n",
            "discriminator loss at step 0: 0.59\n",
            "adversarial loss at step 0: 1.12\n",
            "discriminator loss at step 200: 0.59\n",
            "adversarial loss at step 200: 1.08\n",
            "discriminator loss at step 400: 0.57\n",
            "adversarial loss at step 400: 1.11\n",
            "discriminator loss at step 600: 0.58\n",
            "adversarial loss at step 600: 1.07\n",
            "discriminator loss at step 800: 0.56\n",
            "adversarial loss at step 800: 1.04\n",
            "discriminator loss at step 1000: 0.61\n",
            "adversarial loss at step 1000: 0.98\n",
            "\n",
            "Start epoch 11\n",
            "discriminator loss at step 0: 0.57\n",
            "adversarial loss at step 0: 1.18\n",
            "discriminator loss at step 200: 0.60\n",
            "adversarial loss at step 200: 1.06\n",
            "discriminator loss at step 400: 0.61\n",
            "adversarial loss at step 400: 1.08\n",
            "discriminator loss at step 600: 0.63\n",
            "adversarial loss at step 600: 0.88\n",
            "discriminator loss at step 800: 0.61\n",
            "adversarial loss at step 800: 1.09\n",
            "discriminator loss at step 1000: 0.62\n",
            "adversarial loss at step 1000: 0.89\n",
            "\n",
            "Start epoch 12\n",
            "discriminator loss at step 0: 0.60\n",
            "adversarial loss at step 0: 1.25\n",
            "discriminator loss at step 200: 0.50\n",
            "adversarial loss at step 200: 0.96\n",
            "discriminator loss at step 400: 0.58\n",
            "adversarial loss at step 400: 1.12\n",
            "discriminator loss at step 600: 0.56\n",
            "adversarial loss at step 600: 1.05\n",
            "discriminator loss at step 800: 0.58\n",
            "adversarial loss at step 800: 1.01\n",
            "discriminator loss at step 1000: 0.62\n",
            "adversarial loss at step 1000: 0.88\n",
            "\n",
            "Start epoch 13\n",
            "discriminator loss at step 0: 0.60\n",
            "adversarial loss at step 0: 1.09\n",
            "discriminator loss at step 200: 0.59\n",
            "adversarial loss at step 200: 1.07\n",
            "discriminator loss at step 400: 0.56\n",
            "adversarial loss at step 400: 0.88\n",
            "discriminator loss at step 600: 0.52\n",
            "adversarial loss at step 600: 1.10\n",
            "discriminator loss at step 800: 0.60\n",
            "adversarial loss at step 800: 1.11\n",
            "discriminator loss at step 1000: 0.57\n",
            "adversarial loss at step 1000: 1.03\n",
            "\n",
            "Start epoch 14\n",
            "discriminator loss at step 0: 0.61\n",
            "adversarial loss at step 0: 1.21\n",
            "discriminator loss at step 200: 0.64\n",
            "adversarial loss at step 200: 0.97\n",
            "discriminator loss at step 400: 0.64\n",
            "adversarial loss at step 400: 1.12\n",
            "discriminator loss at step 600: 0.55\n",
            "adversarial loss at step 600: 0.99\n",
            "discriminator loss at step 800: 0.60\n",
            "adversarial loss at step 800: 1.14\n",
            "discriminator loss at step 1000: 0.65\n",
            "adversarial loss at step 1000: 0.94\n",
            "\n",
            "Start epoch 15\n",
            "discriminator loss at step 0: 0.53\n",
            "adversarial loss at step 0: 1.30\n",
            "discriminator loss at step 200: 0.61\n",
            "adversarial loss at step 200: 0.86\n",
            "discriminator loss at step 400: 0.57\n",
            "adversarial loss at step 400: 0.96\n",
            "discriminator loss at step 600: 0.61\n",
            "adversarial loss at step 600: 0.82\n",
            "discriminator loss at step 800: 0.59\n",
            "adversarial loss at step 800: 1.03\n",
            "discriminator loss at step 1000: 0.60\n",
            "adversarial loss at step 1000: 0.84\n",
            "\n",
            "Start epoch 16\n",
            "discriminator loss at step 0: 0.55\n",
            "adversarial loss at step 0: 1.14\n",
            "discriminator loss at step 200: 0.64\n",
            "adversarial loss at step 200: 1.01\n",
            "discriminator loss at step 400: 0.59\n",
            "adversarial loss at step 400: 0.94\n",
            "discriminator loss at step 600: 0.60\n",
            "adversarial loss at step 600: 0.98\n",
            "discriminator loss at step 800: 0.66\n",
            "adversarial loss at step 800: 0.98\n",
            "discriminator loss at step 1000: 0.61\n",
            "adversarial loss at step 1000: 1.04\n",
            "\n",
            "Start epoch 17\n",
            "discriminator loss at step 0: 0.63\n",
            "adversarial loss at step 0: 1.15\n",
            "discriminator loss at step 200: 0.62\n",
            "adversarial loss at step 200: 0.89\n",
            "discriminator loss at step 400: 0.59\n",
            "adversarial loss at step 400: 0.88\n",
            "discriminator loss at step 600: 0.60\n",
            "adversarial loss at step 600: 1.09\n",
            "discriminator loss at step 800: 0.62\n",
            "adversarial loss at step 800: 1.18\n",
            "discriminator loss at step 1000: 0.57\n",
            "adversarial loss at step 1000: 0.92\n",
            "\n",
            "Start epoch 18\n",
            "discriminator loss at step 0: 0.59\n",
            "adversarial loss at step 0: 1.14\n",
            "discriminator loss at step 200: 0.62\n",
            "adversarial loss at step 200: 0.77\n",
            "discriminator loss at step 400: 0.63\n",
            "adversarial loss at step 400: 0.92\n",
            "discriminator loss at step 600: 0.59\n",
            "adversarial loss at step 600: 1.08\n",
            "discriminator loss at step 800: 0.60\n",
            "adversarial loss at step 800: 0.87\n",
            "discriminator loss at step 1000: 0.64\n",
            "adversarial loss at step 1000: 1.09\n",
            "\n",
            "Start epoch 19\n",
            "discriminator loss at step 0: 0.61\n",
            "adversarial loss at step 0: 1.14\n",
            "discriminator loss at step 200: 0.64\n",
            "adversarial loss at step 200: 0.87\n",
            "discriminator loss at step 400: 0.61\n",
            "adversarial loss at step 400: 1.04\n",
            "discriminator loss at step 600: 0.58\n",
            "adversarial loss at step 600: 1.12\n",
            "discriminator loss at step 800: 0.59\n",
            "adversarial loss at step 800: 1.11\n",
            "discriminator loss at step 1000: 0.62\n",
            "adversarial loss at step 1000: 1.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "pWExKop3hfRG",
        "outputId": "4d3a3f6b-750b-49c8-f2bf-c302e5d05e24"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "steps = ['0', '200', '400', '600', '800', '1000']\n",
        "\n",
        "fig, ax = plt.subplots(2, 3, figsize=(10, 10))\n",
        "for idx, step in enumerate(steps):\n",
        "  img = mpimg.imread(f'generated_img{step}.png')\n",
        "  ax[idx // 3][idx % 3].imshow(img, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}