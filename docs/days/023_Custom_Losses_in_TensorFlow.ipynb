{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom Losses in TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQ1mfmVDoqNUo1Kb9PbB8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravimashru/100-days-of-deep-learning/blob/master/docs/days/023_Custom_Losses_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGazQBQmiEOX"
      },
      "source": [
        "# Custom Losses in TensorFlow\n",
        "\n",
        "TensorFlow has implementations of all popular loss functions you would need to use most of the time.\n",
        "\n",
        "However, once in a while, you may need to create your own loss function - either to experiement, do research of your own, or implement a new loss function that you read about in a research paper.\n",
        "\n",
        "There are two ways of creating custom losses in TensorFlow: as a function and as a class (that subclasses `tf.keras.losses.Loss`).\n",
        "\n",
        "In this notebook, we'll see how to use both methods to create a custom Huber loss (Disclosure: TensorFlow already has [an implementation for the Huber loss](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/losses.py#L1095-L1159): `tf.keras.losses.Huber`. However, we'll pretend it doesn't exist so that we can learn how to create our own custom losses).\n",
        "\n",
        "## The Huber Loss\n",
        "\n",
        "[The Huber loss](https://en.wikipedia.org/wiki/Huber_loss) between actual and predicted labels is like a hybrid of a squared loss and a linear loss.\n",
        "\n",
        "If the absolute difference between the true and predicted label is below a predefined **threshold**, then it behaves like squared loss and if it is above the **threshold** it behaves like linear loss.\n",
        "\n",
        "Formally,\n",
        "\n",
        "$$\n",
        "\\text{HuberLoss}(y_{true}, y_{pred}) = \n",
        "\\begin{cases}\n",
        "  \\frac{1}{2} \\cdot {(y_{true} - y_{pred})}^2,\n",
        "  & \\text{if } y_{true} - y_{pred} < \\delta\\\\\n",
        "  \\delta |y_{true} - y_{pred}| - \\frac{1}{2} \\delta^2, & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $\\delta$ is the threshold.\n",
        "\n",
        "## Create a baseline\n",
        "\n",
        "We will use the inbuilt mean square error loss function to create a baseline model on the California housing dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1BITGQbiAAY",
        "outputId": "a37e03bf-c451-4e31-8a06-cb80015f8ce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('TensorFlow: ', tf.__version__)\n",
        "print('Keras: ', tf.keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow:  2.3.0\n",
            "Keras:  2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqoTi9ojpkAn",
        "outputId": "0b91c02c-bb3f-4133-a5b3-c183baf1412e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# This dataset contains only numerical features and has no missing values\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "# Train/validation/test split\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
        "\n",
        "# Create and train model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "print(mse_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 3ms/step - loss: 10778.4756 - val_loss: 13.2174\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 54.5468 - val_loss: 6.1404\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 39.4096 - val_loss: 3.8407\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 29.7426 - val_loss: 2.3915\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 18.7482 - val_loss: 2.5969\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 11.7517 - val_loss: 1.3069\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 7.4931 - val_loss: 1.2469\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 4.5499 - val_loss: 0.9910\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.4598 - val_loss: 0.9339\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.3431 - val_loss: 0.8204\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.1239 - val_loss: 0.8065\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8383 - val_loss: 0.6969\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8507 - val_loss: 0.9048\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.8007 - val_loss: 0.9192\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.7622 - val_loss: 0.6340\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.7787 - val_loss: 0.6911\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0860 - val_loss: 1.5543\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9226 - val_loss: 0.8010\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 1.0577 - val_loss: 0.7525\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 2.0647 - val_loss: 10.9900\n",
            "162/162 [==============================] - 0s 759us/step - loss: 10.9444\n",
            "10.944363594055176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGRYvUY0pELI"
      },
      "source": [
        "## Create a custom loss function\n",
        "\n",
        "Using a custom loss function in TensorFlow is as easy as creating a function that has two parameters: $y_{true}$ and $y_{pred}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvW-egg0pEuE",
        "outputId": "edeb80bb-f2e3-48bd-b8af-2beff51e881a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define custom Huber loss function\n",
        "# Takes threshold as a parameter\n",
        "# Returns function that calculates Huber loss with given threshold\n",
        "def make_huber_loss(threshold=1.0):\n",
        "\n",
        "  def huber_loss_fn(y_true, y_pred):\n",
        "\n",
        "    # Calculate difference between actual and predicted\n",
        "    delta = y_true - y_pred\n",
        "\n",
        "    # Mark which differences are below threshold\n",
        "    below_threshold = tf.abs(delta) < threshold\n",
        "\n",
        "    # Calculated squared loss (used for delta < threshold)\n",
        "    squared_loss = 0.5 * tf.square(delta)\n",
        "\n",
        "    # Calculate linear loss (used for delta > threshold)\n",
        "    linear_loss = threshold * tf.abs(delta) - 0.5 * threshold**2\n",
        "\n",
        "    # Return loss based on delta and threshold\n",
        "    return tf.where(below_threshold, squared_loss, linear_loss)\n",
        "\n",
        "  return huber_loss_fn\n",
        "\n",
        "# Use the custom loss function to train the model\n",
        "custom_huber_loss = make_huber_loss()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=custom_huber_loss, optimizer=\"adam\")\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "print(mse_test)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 7.1424 - val_loss: 0.5724\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6046 - val_loss: 0.5325\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5913 - val_loss: 0.3536\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5497 - val_loss: 0.3919\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5500 - val_loss: 0.6900\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5480 - val_loss: 0.3744\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3940 - val_loss: 0.5724\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5156 - val_loss: 0.2885\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4469 - val_loss: 0.2729\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5020 - val_loss: 0.3772\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5190 - val_loss: 0.2655\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.2469\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5617 - val_loss: 0.2852\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.2873\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5220 - val_loss: 0.3302\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.3453\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4777 - val_loss: 0.8006\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4736 - val_loss: 0.2488\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4928 - val_loss: 0.3813\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4365 - val_loss: 0.2876\n",
            "162/162 [==============================] - 0s 860us/step - loss: 0.2983\n",
            "0.29831212759017944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VvtsXKhqZ3M"
      },
      "source": [
        "## Create a custom loss class\n",
        "\n",
        "To create a custom loss class, you have to subclass `tf.keras.losses.Loss`. You need to override the `call` method with your custom loss function code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPe4pk5xrq59",
        "outputId": "fb674ade-ae43-468f-8cd5-03e7105138fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create custom loss class\n",
        "class CustomHuberLoss(tf.keras.losses.Loss):\n",
        "\n",
        "  def __init__(self, threshold=1.0, **kwargs):\n",
        "    self.threshold = threshold\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "  # This method calculates and returns the loss\n",
        "  def call(self, y_true, y_pred):\n",
        "    delta = y_true - y_pred\n",
        "    below_threshold = tf.abs(delta) < self.threshold\n",
        "    squared_loss = 0.5 * tf.square(delta)\n",
        "    linear_loss = self.threshold * tf.abs(delta) - 0.5 * self.threshold**2\n",
        "    return tf.where(below_threshold, squared_loss, linear_loss)\n",
        "\n",
        "# Use the custom loss function to train the model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=CustomHuberLoss(), optimizer=\"adam\")\n",
        "\n",
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "print(mse_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 7.5532 - val_loss: 0.4532\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5518 - val_loss: 0.8375\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4469 - val_loss: 0.7997\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5783 - val_loss: 0.3615\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3717 - val_loss: 0.3331\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4114 - val_loss: 0.3781\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4651 - val_loss: 0.3141\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4824 - val_loss: 0.2808\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4401 - val_loss: 0.4061\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4628 - val_loss: 1.0965\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4576 - val_loss: 0.2506\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4357\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4073 - val_loss: 0.2753\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4560 - val_loss: 0.6995\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.2986\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4457 - val_loss: 0.4471\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5314 - val_loss: 0.4798\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3768 - val_loss: 0.2533\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4387 - val_loss: 0.4340\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.2676\n",
            "162/162 [==============================] - 0s 785us/step - loss: 0.2770\n",
            "0.2769745886325836\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}